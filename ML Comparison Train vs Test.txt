
GBM:
gbm = GradientBoostingClassifier(n_estimators=500, subsample=0.70, max_features=0.06, validation_fraction=0.1, n_iter_no_change=10, verbose=2, random_state=SEED)

On Train:
>>> print("\nF1 Score = {:.5f}".format(f1_score(y_val, pred_val, average="micro")))

F1 Score = 0.74583
>>> print("\nClassification Report:")

Classification Report:
>>> print(classification_report(y_val, pred_val))
              precision    recall  f1-score   support

           0       0.69      0.84      0.76       231
           1       0.82      0.65      0.73       249

    accuracy                           0.75       480
   macro avg       0.76      0.75      0.74       480
weighted avg       0.76      0.75      0.74       480


On Test:
F1 Score = 0.68500
>>> print("\nClassification Report:")

Classification Report:
>>> print(classification_report(y_test, pred_val))
              precision    recall  f1-score   support

           0       0.61      0.92      0.74       287
           1       0.86      0.47      0.61       313

    accuracy                           0.69       600
   macro avg       0.74      0.69      0.67       600
weighted avg       0.74      0.69      0.67       600


With Feature Engineering:

On Train
>>> print("\nF1 Score = {:.5f}".format(f1_score(y_val, pred_val, average="micro")))

F1 Score = 0.73333
>>> print("\nClassification Report:")

Classification Report:
>>>
>>> print(classification_report(y_val, pred_val))
              precision    recall  f1-score   support

           0       0.66      0.88      0.76       112
           1       0.86      0.60      0.71       128

    accuracy                           0.73       240
   macro avg       0.76      0.74      0.73       240
weighted avg       0.76      0.73      0.73       240


On Test
>>> print("\nF1 Score = {:.5f}".format(f1_score(y_test, pred_test, average="micro")))

F1 Score = 0.64667
>>> print("\nClassification Report:")

Classification Report:
>>> print(classification_report(y_test, pred_test))
              precision    recall  f1-score   support

           0       0.58      0.92      0.71       287
           1       0.84      0.40      0.54       313

    accuracy                           0.65       600
   macro avg       0.71      0.66      0.63       600
weighted avg       0.72      0.65      0.62       600






Light GBM:
lgbm = LGBMClassifier(n_estimators=1500, feature_fraction=0.07, bagging_fraction=0.67, bagging_freq=1, verbose=2, n_jobs=3, random_state=SEED)

On Train:
>>> print("\nF1 Score = {:.5f}".format(f1_score(y_val, pred_val, average="micro")))

F1 Score = 0.63333
>>> print("\nClassification Report:")

Classification Report:
>>>
>>> print(classification_report(y_val, pred_val))
              precision    recall  f1-score   support

           0       0.59      0.75      0.66       231
           1       0.70      0.52      0.60       249

    accuracy                           0.63       480
   macro avg       0.64      0.64      0.63       480
weighted avg       0.65      0.63      0.63       480


On Test:
>>> print("\nF1 Score = {:.5f}".format(f1_score(y_test, pred_val, average="micro")))

F1 Score = 0.62833
>>> print("\nClassification Report:")

Classification Report:
>>> print(classification_report(y_test, pred_val))
              precision    recall  f1-score   support

           0       0.58      0.81      0.68       287
           1       0.73      0.46      0.56       313

    accuracy                           0.63       600
   macro avg       0.65      0.64      0.62       600
weighted avg       0.66      0.63      0.62       600


XGBoosting (xgb)
xgb = XGBClassifier(n_estimators=1500, tree_method='hist', subsample=0.67, colsample_level=0.06, verbose=2, n_jobs=3, random_state=SEED)

On Train:
>>> print("\nF1 Score = {:.5f}".format(f1_score(y_val, pred_val, average="micro")))

F1 Score = 0.71250
>>> print("\nClassification Report:")

Classification Report:
>>> 
>>> print(classification_report(y_val, pred_val))
              precision    recall  f1-score   support

           0       0.68      0.73      0.70       112
           1       0.75      0.70      0.72       128

    accuracy                           0.71       240
   macro avg       0.71      0.71      0.71       240
weighted avg       0.72      0.71      0.71       240


On Test:
>>> print("\nF1 Score = {:.5f}".format(f1_score(y_test, pred_val, average="micro")))

F1 Score = 0.67167
>>> print("\nClassification Report:")

Classification Report:
>>> print(classification_report(y_test, pred_val))
              precision    recall  f1-score   support

           0       0.62      0.79      0.70       287
           1       0.74      0.57      0.64       313

    accuracy                           0.67       600
   macro avg       0.68      0.68      0.67       600
weighted avg       0.69      0.67      0.67       600



With Feature Engineering

On Train

>>> print("\nF1 Score = {:.5f}".format(f1_score(y_val, pred_val, average="micro")))

F1 Score = 0.64167
>>> print("\nClassification Report:")

Classification Report:
>>>
>>> print(classification_report(y_val, pred_val))
              precision    recall  f1-score   support

           0       0.60      0.67      0.64       112
           1       0.68      0.62      0.65       128

    accuracy                           0.64       240
   macro avg       0.64      0.64      0.64       240
weighted avg       0.65      0.64      0.64       240



On Test
>>> print("\nF1 Score = {:.5f}".format(f1_score(y_test, pred_test, average="micro")))

F1 Score = 0.62000
>>> print("\nClassification Report:")

Classification Report:
>>> print(classification_report(y_test, pred_test))
              precision    recall  f1-score   support

           0       0.59      0.69      0.63       287
           1       0.66      0.56      0.61       313

    accuracy                           0.62       600
   macro avg       0.62      0.62      0.62       600
weighted avg       0.63      0.62      0.62       600







KNN

On Train:
>>> print("\nF1 Score = {:.5f}".format(f1_score(y_val, pred_val, average="micro")))

F1 Score = 0.66667
>>> print("\nClassification Report:")

Classification Report:
>>>
>>> print(classification_report(y_val, pred_val))
              precision    recall  f1-score   support

           0       0.60      0.83      0.70       112
           1       0.78      0.52      0.63       128

    accuracy                           0.67       240
   macro avg       0.69      0.68      0.66       240
weighted avg       0.70      0.67      0.66       240


On Test:
>>> print("\nF1 Score = {:.5f}".format(f1_score(y_test, pred_val, average="micro")))

F1 Score = 0.52833
>>> print("\nClassification Report:")

Classification Report:
>>> print(classification_report(y_test, pred_val))
              precision    recall  f1-score   support

           0       0.50      0.79      0.61       287
           1       0.60      0.29      0.39       313

    accuracy                           0.53       600
   macro avg       0.55      0.54      0.50       600
weighted avg       0.55      0.53      0.50       600



With feature engineering & KNN(3):

On Train

Confusion matrix:
>>> print(confusion_matrix(y_val, pred_val))
[[107   5]
 [ 77  51]]
>>> print("\nF1 Score = {:.5f}".format(f1_score(y_val, pred_val, average="micro")))

F1 Score = 0.65833
>>> print("\nClassification Report:")

Classification Report:
>>>
>>> print(classification_report(y_val, pred_val))
              precision    recall  f1-score   support

           0       0.58      0.96      0.72       112
           1       0.91      0.40      0.55       128

    accuracy                           0.66       240
   macro avg       0.75      0.68      0.64       240
weighted avg       0.76      0.66      0.63       240


On Test

>>> print("\nF1 Score = {:.5f}".format(f1_score(y_test, pred_test, average="micro")))

F1 Score = 0.54000
>>> print("\nClassification Report:")

Classification Report:
>>> print(classification_report(y_test, pred_test))
              precision    recall  f1-score   support

           0       0.51      0.95      0.67       287
           1       0.79      0.16      0.27       313

    accuracy                           0.54       600
   macro avg       0.65      0.56      0.47       600
weighted avg       0.66      0.54      0.46       600


SVC

On Train:
>>> print("\nF1 Score = {:.5f}".format(f1_score(y_val, pred_val, average="micro")))

F1 Score = 0.79167
>>> print("\nClassification Report:")

Classification Report:
>>> 
>>> print(classification_report(y_val, pred_val))
              precision    recall  f1-score   support

           0       0.75      0.84      0.79       112
           1       0.84      0.75      0.79       128

    accuracy                           0.79       240
   macro avg       0.79      0.79      0.79       240
weighted avg       0.80      0.79      0.79       240


On Test:
>>> print("\nF1 Score = {:.5f}".format(f1_score(y_test, pred_val, average="micro")))

F1 Score = 0.70167
>>> print("\nClassification Report:")

Classification Report:
>>> print(classification_report(y_test, pred_val))
              precision    recall  f1-score   support

           0       0.64      0.85      0.73       287
           1       0.81      0.56      0.66       313

    accuracy                           0.70       600
   macro avg       0.72      0.71      0.70       600
weighted avg       0.73      0.70      0.70       600


With feature engineering

On Train

>>> print("\nF1 Score = {:.5f}".format(f1_score(y_val, pred_val, average="micro")))

F1 Score = 0.78333
>>> print("\nClassification Report:")

Classification Report:
>>>
>>> print(classification_report(y_val, pred_val))
              precision    recall  f1-score   support

           0       0.73      0.85      0.79       112
           1       0.85      0.73      0.78       128

    accuracy                           0.78       240
   macro avg       0.79      0.79      0.78       240
weighted avg       0.79      0.78      0.78       240

On Test

>>> print("\nF1 Score = {:.5f}".format(f1_score(y_test, pred_test, average="micro")))

F1 Score = 0.69000
>>> print("\nClassification Report:")

Classification Report:
>>> print(classification_report(y_test, pred_test))
              precision    recall  f1-score   support

           0       0.62      0.89      0.73       287
           1       0.83      0.51      0.63       313

    accuracy                           0.69       600
   macro avg       0.73      0.70      0.68       600
weighted avg       0.73      0.69      0.68       600





Linear SVM

On Train:
>>> print("\nF1 Score = {:.5f}".format(f1_score(y_val, pred_val, average="micro")))

F1 Score = 0.79583
>>> print("\nClassification Report:")

Classification Report:
>>>
>>> print(classification_report(y_val, pred_val))
              precision    recall  f1-score   support

           0       0.74      0.86      0.80       112
           1       0.86      0.74      0.79       128

    accuracy                           0.80       240
   macro avg       0.80      0.80      0.80       240
weighted avg       0.80      0.80      0.80       240


On Test:
>>> print("\nF1 Score = {:.5f}".format(f1_score(y_test, pred_val, average="micro")))

F1 Score = 0.70000
>>> print("\nClassification Report:")

Classification Report:
>>> print(classification_report(y_test, pred_val))
              precision    recall  f1-score   support

           0       0.65      0.82      0.72       287
           1       0.78      0.59      0.67       313

    accuracy                           0.70       600
   macro avg       0.71      0.70      0.70       600
weighted avg       0.72      0.70      0.70       600



vectorizer2 = TfidfVectorizer(preprocessor=preprocess_text, max_features=10000, ngram_range=[1,3], max_df=0.5, min_df=0.001, use_idf=True)

On Train

>>> print("\nF1 Score = {:.5f}".format(f1_score(y_val, pred_val, average="micro")))

F1 Score = 0.79583
>>> print("\nClassification Report:")

Classification Report:
>>>
>>> print(classification_report(y_val, pred_val))
              precision    recall  f1-score   support

           0       0.74      0.86      0.80       112
           1       0.86      0.74      0.79       128

    accuracy                           0.80       240
   macro avg       0.80      0.80      0.80       240
weighted avg       0.80      0.80      0.80       240

On Test

>>> print("\nF1 Score = {:.5f}".format(f1_score(y_test, pred_val, average="micro")))

F1 Score = 0.69333
>>> print("\nClassification Report:")

Classification Report:
>>> print(classification_report(y_test, pred_val))
              precision    recall  f1-score   support

           0       0.64      0.81      0.72       287
           1       0.77      0.59      0.67       313

    accuracy                           0.69       600
   macro avg       0.71      0.70      0.69       600
weighted avg       0.71      0.69      0.69       600


With feature engineering:

On Train 

>>> print("\nF1 Score = {:.5f}".format(f1_score(y_val, pred_val, average="micro")))

F1 Score = 0.77500
>>> print("\nClassification Report:")

Classification Report:
>>>
>>> print(classification_report(y_val, pred_val))
              precision    recall  f1-score   support

           0       0.71      0.88      0.79       112
           1       0.87      0.68      0.76       128

    accuracy                           0.78       240
   macro avg       0.79      0.78      0.77       240
weighted avg       0.79      0.78      0.77       240


On Test

>>> print("\nF1 Score = {:.5f}".format(f1_score(y_test, pred_test, average="micro")))

F1 Score = 0.66667
>>> print("\nClassification Report:")

Classification Report:
>>> print(classification_report(y_test, pred_test))
              precision    recall  f1-score   support

           0       0.60      0.91      0.72       287
           1       0.85      0.44      0.58       313

    accuracy                           0.67       600
   macro avg       0.72      0.68      0.65       600
weighted avg       0.73      0.67      0.65       600


BernoulliNB (bnb)

On Train:
>>> print("\nF1 Score = {:.5f}".format(f1_score(y_val, pred_val, average="micro")))

F1 Score = 0.78333
>>> print("\nClassification Report:")

Classification Report:
>>>
>>> print(classification_report(y_val, pred_val))
              precision    recall  f1-score   support

           0       0.76      0.78      0.77       112
           1       0.80      0.79      0.80       128

    accuracy                           0.78       240
   macro avg       0.78      0.78      0.78       240
weighted avg       0.78      0.78      0.78       240


On Test:
>>> print("\nF1 Score = {:.5f}".format(f1_score(y_test, pred_val, average="micro")))

F1 Score = 0.72667
>>> print("\nClassification Report:")

Classification Report:
>>> print(classification_report(y_test, pred_val))
              precision    recall  f1-score   support

           0       0.67      0.85      0.75       287
           1       0.81      0.62      0.70       313

    accuracy                           0.73       600
   macro avg       0.74      0.73      0.72       600
weighted avg       0.75      0.73      0.72       600


bnb = BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)
On Train
>>> print("\nF1 Score = {:.5f}".format(f1_score(y_val, pred_val, average="micro")))

F1 Score = 0.78333
>>> print("\nClassification Report:")

Classification Report:
>>>
>>> print(classification_report(y_val, pred_val))
              precision    recall  f1-score   support

           0       0.76      0.78      0.77       112
           1       0.80      0.79      0.80       128

    accuracy                           0.78       240
   macro avg       0.78      0.78      0.78       240
weighted avg       0.78      0.78      0.78       240

On Test
>>> print("\nF1 Score = {:.5f}".format(f1_score(y_test, pred_val, average="micro")))

F1 Score = 0.72500
>>> print("\nClassification Report:")

Classification Report:
>>> print(classification_report(y_test, pred_val))
              precision    recall  f1-score   support

           0       0.67      0.85      0.75       287
           1       0.81      0.61      0.70       313

    accuracy                           0.73       600
   macro avg       0.74      0.73      0.72       600
weighted avg       0.74      0.72      0.72       600




vectorizer2 = TfidfVectorizer(preprocessor=preprocess_text, max_features=2000, ngram_range=[1,2], max_df=0.5, min_df=0.001, use_idf=True)
bnb = BernoulliNB()

On Train

>>> print("\nF1 Score = {:.5f}".format(f1_score(y_val, pred_val, average="micro")))

F1 Score = 0.78333
>>> print("\nClassification Report:")

Classification Report:
>>>
>>> print(classification_report(y_val, pred_val))
              precision    recall  f1-score   support

           0       0.76      0.78      0.77       112
           1       0.80      0.79      0.80       128

    accuracy                           0.78       240
   macro avg       0.78      0.78      0.78       240
weighted avg       0.78      0.78      0.78       240


On Test

>>> print("\nF1 Score = {:.5f}".format(f1_score(y_test, pred_val, average="micro")))

F1 Score = 0.72333
>>> print("\nClassification Report:")

Classification Report:
>>> print(classification_report(y_test, pred_val))
              precision    recall  f1-score   support

           0       0.67      0.85      0.75       287
           1       0.81      0.61      0.70       313

    accuracy                           0.72       600
   macro avg       0.74      0.73      0.72       600
weighted avg       0.74      0.72      0.72       600




vectorizer2 = TfidfVectorizer(preprocessor=preprocess_text, max_features=10000, ngram_range=[1,3], max_df=0.5, min_df=0.001, use_idf=True)
bnb = BernoulliNB()

On Train
>>> print("\nF1 Score = {:.5f}".format(f1_score(y_val, pred_val, average="micro")))

F1 Score = 0.78333
>>> print("\nClassification Report:")

Classification Report:
>>>
>>> print(classification_report(y_val, pred_val))
              precision    recall  f1-score   support

           0       0.76      0.78      0.77       112
           1       0.80      0.79      0.80       128

    accuracy                           0.78       240
   macro avg       0.78      0.78      0.78       240
weighted avg       0.78      0.78      0.78       240

On Test
>>> print("\nF1 Score = {:.5f}".format(f1_score(y_test, pred_val, average="micro")))

F1 Score = 0.72500
>>> print("\nClassification Report:")

Classification Report:
>>> print(classification_report(y_test, pred_val))
              precision    recall  f1-score   support

           0       0.67      0.85      0.75       287
           1       0.81      0.61      0.70       313

    accuracy                           0.73       600
   macro avg       0.74      0.73      0.72       600
weighted avg       0.74      0.72      0.72       600



vectorizer2 = TfidfVectorizer(preprocessor=preprocess_text, ngram_range=[1,3], max_df=0.5, min_df=0.001, use_idf=True)
bnb = BernoulliNB()

On Train

>>> print("\nF1 Score = {:.5f}".format(f1_score(y_val, pred_val, average="micro")))

F1 Score = 0.78333
>>> print("\nClassification Report:")

Classification Report:
>>>
>>> print(classification_report(y_val, pred_val))
              precision    recall  f1-score   support

           0       0.76      0.78      0.77       112
           1       0.80      0.79      0.80       128

    accuracy                           0.78       240
   macro avg       0.78      0.78      0.78       240
weighted avg       0.78      0.78      0.78       240

On Test

>>> print("\nF1 Score = {:.5f}".format(f1_score(y_test, pred_val, average="micro")))

F1 Score = 0.72500
>>> print("\nClassification Report:")

Classification Report:
>>> print(classification_report(y_test, pred_val))
              precision    recall  f1-score   support

           0       0.67      0.85      0.75       287
           1       0.81      0.61      0.70       313

    accuracy                           0.73       600
   macro avg       0.74      0.73      0.72       600
weighted avg       0.74      0.72      0.72       600




vectorizer2 = TfidfVectorizer(preprocessor=preprocess_text, max_features=1000, ngram_range=[1,3], max_df=0.5, min_df=0.001, use_idf=True)
bnb = BernoulliNB()

On Train

>>> print("\nF1 Score = {:.5f}".format(f1_score(y_val, pred_val, average="micro")))

F1 Score = 0.78333
>>> print("\nClassification Report:")

Classification Report:
>>>
>>> print(classification_report(y_val, pred_val))
              precision    recall  f1-score   support

           0       0.76      0.78      0.77       112
           1       0.80      0.79      0.80       128

    accuracy                           0.78       240
   macro avg       0.78      0.78      0.78       240
weighted avg       0.78      0.78      0.78       240


On Test

>>> print("\nF1 Score = {:.5f}".format(f1_score(y_test, pred_val, average="micro")))

F1 Score = 0.72333
>>> print("\nClassification Report:")

Classification Report:
>>> print(classification_report(y_test, pred_val))
              precision    recall  f1-score   support

           0       0.67      0.84      0.74       287
           1       0.81      0.61      0.70       313

    accuracy                           0.72       600
   macro avg       0.74      0.73      0.72       600
weighted avg       0.74      0.72      0.72       600



vectorizer2 = TfidfVectorizer(preprocessor=preprocess_text, max_features=1500, ngram_range=[1,3], max_df=0.5, min_df=0.001, use_idf=True)
bnb = BernoulliNB()

On Train

>>> print("\nF1 Score = {:.5f}".format(f1_score(y_val, pred_val, average="micro")))

F1 Score = 0.78333
>>> print("\nClassification Report:")

Classification Report:
>>>
>>> print(classification_report(y_val, pred_val))
              precision    recall  f1-score   support

           0       0.76      0.78      0.77       112
           1       0.80      0.79      0.80       128

    accuracy                           0.78       240
   macro avg       0.78      0.78      0.78       240
weighted avg       0.78      0.78      0.78       240

On Test

>>> print("\nF1 Score = {:.5f}".format(f1_score(y_test, pred_val, average="micro")))

F1 Score = 0.72500
>>> print("\nClassification Report:")

Classification Report:
>>> print(classification_report(y_test, pred_val))
              precision    recall  f1-score   support

           0       0.67      0.85      0.75       287
           1       0.81      0.61      0.70       313

    accuracy                           0.73       600
   macro avg       0.74      0.73      0.72       600
weighted avg       0.74      0.72      0.72       600


With Feature Engineering:

On Train

>>> print("\nF1 Score = {:.5f}".format(f1_score(y_val, pred_val, average="micro")))

F1 Score = 0.80417
>>> print("\nClassification Report:")

Classification Report:
>>>
>>> print(classification_report(y_val, pred_val))
              precision    recall  f1-score   support

           0       0.77      0.83      0.80       112
           1       0.84      0.78      0.81       128

    accuracy                           0.80       240
   macro avg       0.80      0.81      0.80       240
weighted avg       0.81      0.80      0.80       240

On Test

>>> print("\nF1 Score = {:.5f}".format(f1_score(y_test, pred_val, average="micro")))

F1 Score = 0.70833
>>> print("\nClassification Report:")

Classification Report:
>>> print(classification_report(y_test, pred_val))
              precision    recall  f1-score   support

           0       0.65      0.86      0.74       287
           1       0.82      0.57      0.67       313

    accuracy                           0.71       600
   macro avg       0.73      0.71      0.70       600
weighted avg       0.74      0.71      0.70       600


With feature engineering without exclamation num:
On Train
>>> print("\nF1 Score = {:.5f}".format(f1_score(y_val, pred_val, average="micro")))

F1 Score = 0.77083
>>> print("\nClassification Report:")

Classification Report:
>>>
>>> print(classification_report(y_val, pred_val))
              precision    recall  f1-score   support

           0       0.72      0.82      0.77       112
           1       0.82      0.73      0.77       128

    accuracy                           0.77       240
   macro avg       0.77      0.77      0.77       240
weighted avg       0.78      0.77      0.77       240

On Test
>>> print("\nF1 Score = {:.5f}".format(f1_score(y_test, pred_val, average="micro")))

F1 Score = 0.71167
>>> print("\nClassification Report:")

Classification Report:
>>> print(classification_report(y_test, pred_val))
              precision    recall  f1-score   support

           0       0.65      0.87      0.74       287
           1       0.83      0.56      0.67       313

    accuracy                           0.71       600
   macro avg       0.74      0.72      0.71       600
weighted avg       0.74      0.71      0.71       600


Feature Engineering v2

On Train
>>> print("\nF1 Score = {:.5f}".format(f1_score(y_val, pred_val, average="micro")))

F1 Score = 0.71250
>>> print("\nClassification Report:")

Classification Report:
>>>
>>> print(classification_report(y_val, pred_val))
              precision    recall  f1-score   support

           0       0.67      0.76      0.71       112
           1       0.76      0.67      0.71       128

    accuracy                           0.71       240
   macro avg       0.72      0.72      0.71       240
weighted avg       0.72      0.71      0.71       240


On Test
>>> print("\nF1 Score = {:.5f}".format(f1_score(y_test, pred_test, average="micro")))

F1 Score = 0.70333
>>> print("\nClassification Report:")

Classification Report:
>>> print(classification_report(y_test, pred_test))
              precision    recall  f1-score   support

           0       0.65      0.82      0.73       287
           1       0.78      0.60      0.68       313

    accuracy                           0.70       600
   macro avg       0.72      0.71      0.70       600
weighted avg       0.72      0.70      0.70       600


MultinomialNB()

On Train
>>> print("\nF1 Score = {:.5f}".format(f1_score(y_val, pred_val, average="micro")))

F1 Score = 0.77083
>>> print("\nClassification Report:")

Classification Report:
>>>
>>> print(classification_report(y_val, pred_val))
              precision    recall  f1-score   support

           0       0.75      0.76      0.76       112
           1       0.79      0.78      0.78       128

    accuracy                           0.77       240
   macro avg       0.77      0.77      0.77       240
weighted avg       0.77      0.77      0.77       240


On Test
>>> print("\nF1 Score = {:.5f}".format(f1_score(y_test, pred_val, average="micro")))

F1 Score = 0.72500
>>> print("\nClassification Report:")

Classification Report:
>>> print(classification_report(y_test, pred_val))
              precision    recall  f1-score   support

           0       0.67      0.85      0.75       287
           1       0.82      0.61      0.70       313

    accuracy                           0.73       600
   macro avg       0.74      0.73      0.72       600
weighted avg       0.75      0.72      0.72       600



Decision Tree (dt)

On Train
>>> print("\nF1 Score = {:.5f}".format(f1_score(y_val, pred_val, average="micro")))

F1 Score = 0.57500
>>> print("\nClassification Report:")

Classification Report:
>>> 
>>> print(classification_report(y_val, pred_val))
              precision    recall  f1-score   support

           0       0.52      0.97      0.68       112
           1       0.91      0.23      0.36       128

    accuracy                           0.57       240
   macro avg       0.72      0.60      0.52       240
weighted avg       0.73      0.57      0.51       240


On Test
>>> print("\nF1 Score = {:.5f}".format(f1_score(y_test, pred_val, average="micro")))

F1 Score = 0.53000
>>> print("\nClassification Report:")

Classification Report:
>>> print(classification_report(y_test, pred_val))
              precision    recall  f1-score   support

           0       0.50      0.96      0.66       287
           1       0.79      0.13      0.23       313

    accuracy                           0.53       600
   macro avg       0.65      0.55      0.45       600
weighted avg       0.65      0.53      0.44       600



Logistic Regression (LR)

On Train
>>> print("\nF1 Score = {:.5f}".format(f1_score(y_val, pred_val, average="micro")))

F1 Score = 0.78750
>>> print("\nClassification Report:")

Classification Report:
>>>
>>> print(classification_report(y_val, pred_val))
              precision    recall  f1-score   support

           0       0.75      0.82      0.78       112
           1       0.83      0.76      0.79       128

    accuracy                           0.79       240
   macro avg       0.79      0.79      0.79       240
weighted avg       0.79      0.79      0.79       240


On Test
>>> print("\nF1 Score = {:.5f}".format(f1_score(y_test, pred_val, average="micro")))

F1 Score = 0.71167
>>> print("\nClassification Report:")

Classification Report:
>>> print(classification_report(y_test, pred_val))
              precision    recall  f1-score   support

           0       0.65      0.87      0.74       287
           1       0.83      0.57      0.67       313

    accuracy                           0.71       600
   macro avg       0.74      0.72      0.71       600
weighted avg       0.74      0.71      0.71       600


Random Forest

with n_estimator = 300:
On Train
>>> print("\nF1 Score = {:.5f}".format(f1_score(y_val, pred_val, average="micro")))

F1 Score = 0.76667
>>> print("\nClassification Report:")

Classification Report:
>>>
>>> print(classification_report(y_val, pred_val))
              precision    recall  f1-score   support

           0       0.73      0.79      0.76       112
           1       0.80      0.75      0.77       128

    accuracy                           0.77       240
   macro avg       0.77      0.77      0.77       240
weighted avg       0.77      0.77      0.77       240


On Test
>>> print("\nF1 Score = {:.5f}".format(f1_score(y_test, pred_val, average="micro")))

F1 Score = 0.72333
>>> print("\nClassification Report:")

Classification Report:
>>> print(classification_report(y_test, pred_val))
              precision    recall  f1-score   support

           0       0.66      0.87      0.75       287
           1       0.83      0.59      0.69       313

    accuracy                           0.72       600
   macro avg       0.75      0.73      0.72       600
weighted avg       0.75      0.72      0.72       600



with n_estimator = 500:

On Train
>>> print("\nF1 Score = {:.5f}".format(f1_score(y_val, pred_val, average="micro")))

F1 Score = 0.77083
>>> print("\nClassification Report:")

Classification Report:
>>>
>>> print(classification_report(y_val, pred_val))
              precision    recall  f1-score   support

           0       0.73      0.81      0.77       112
           1       0.82      0.73      0.77       128

    accuracy                           0.77       240
   macro avg       0.77      0.77      0.77       240
weighted avg       0.78      0.77      0.77       240


On Test
>>> print("\nF1 Score = {:.5f}".format(f1_score(y_test, pred_val, average="micro")))

F1 Score = 0.72500
>>> print("\nClassification Report:")

Classification Report:
>>> print(classification_report(y_test, pred_val))
              precision    recall  f1-score   support

           0       0.66      0.87      0.75       287
           1       0.84      0.59      0.69       313

    accuracy                           0.73       600
   macro avg       0.75      0.73      0.72       600
weighted avg       0.75      0.72      0.72       600


With feature engineering

On Train

>>> print("\nF1 Score = {:.5f}".format(f1_score(y_val, pred_val, average="micro")))

F1 Score = 0.49167
>>> print("\nClassification Report:")

Classification Report:
>>>
>>> print(classification_report(y_val, pred_val))
              precision    recall  f1-score   support

           0       0.48      1.00      0.65       112
           1       1.00      0.05      0.09       128

    accuracy                           0.49       240
   macro avg       0.74      0.52      0.37       240
weighted avg       0.76      0.49      0.35       240

On Test

>>> print("\nF1 Score = {:.5f}".format(f1_score(y_test, pred_test, average="micro")))

F1 Score = 0.49000
>>> print("\nClassification Report:")

Classification Report:
>>> print(classification_report(y_test, pred_test))
              precision    recall  f1-score   support

           0       0.48      1.00      0.65       287
           1       1.00      0.02      0.04       313

    accuracy                           0.49       600
   macro avg       0.74      0.51      0.35       600
weighted avg       0.75      0.49      0.33       600


AdaBoostClassifier (ab)

On Train
>>> print("\nF1 Score = {:.5f}".format(f1_score(y_val, pred_val, average="micro")))

F1 Score = 0.77917
>>> print("\nClassification Report:")

Classification Report:
>>>
>>> print(classification_report(y_val, pred_val))
              precision    recall  f1-score   support

           0       0.74      0.80      0.77       112
           1       0.82      0.76      0.79       128

    accuracy                           0.78       240
   macro avg       0.78      0.78      0.78       240
weighted avg       0.78      0.78      0.78       240


On Test
>>> print("\nF1 Score = {:.5f}".format(f1_score(y_test, pred_val, average="micro")))

F1 Score = 0.69167
>>> print("\nClassification Report:")

Classification Report:
>>> print(classification_report(y_test, pred_val))
              precision    recall  f1-score   support

           0       0.63      0.85      0.73       287
           1       0.80      0.55      0.65       313

    accuracy                           0.69       600
   macro avg       0.72      0.70      0.69       600
weighted avg       0.72      0.69      0.69       600










MLP (Neural Network)
mlp = MLPClassifier(random_state=150, verbose=2, max_iter=200)

On Train
>>> print("\nF1 Score = {:.5f}".format(f1_score(y_val, pred_val, average="micro")))

F1 Score = 0.75000
>>> print("\nClassification Report:")

Classification Report:
>>>
>>> print(classification_report(y_val, pred_val))
              precision    recall  f1-score   support

           0       0.71      0.79      0.75       112
           1       0.80      0.71      0.75       128

    accuracy                           0.75       240
   macro avg       0.75      0.75      0.75       240
weighted avg       0.76      0.75      0.75       240



On Test
>>> print("\nF1 Score = {:.5f}".format(f1_score(y_test, pred_val, average="micro")))

F1 Score = 0.67333
>>> print("\nClassification Report:")

Classification Report:
>>> print(classification_report(y_test, pred_val))
              precision    recall  f1-score   support

           0       0.63      0.76      0.69       287
           1       0.73      0.60      0.66       313

    accuracy                           0.67       600
   macro avg       0.68      0.68      0.67       600
weighted avg       0.68      0.67      0.67       600



With Feature Engineering v2:
On Train

>>> print("\nF1 Score = {:.5f}".format(f1_score(y_val, pred_val, average="micro")))

F1 Score = 0.79167
>>> print("\nClassification Report:")

Classification Report:
>>>
>>> print(classification_report(y_val, pred_val))
              precision    recall  f1-score   support

           0       0.75      0.83      0.79       112
           1       0.84      0.76      0.80       128

    accuracy                           0.79       240
   macro avg       0.79      0.79      0.79       240
weighted avg       0.80      0.79      0.79       240


On Test

>>> print("\nF1 Score = {:.5f}".format(f1_score(y_test, pred_test, average="micro")))

F1 Score = 0.71000
>>> print("\nClassification Report:")

Classification Report:
>>> print(classification_report(y_test, pred_test))
              precision    recall  f1-score   support

           0       0.65      0.84      0.73       287
           1       0.80      0.59      0.68       313

    accuracy                           0.71       600
   macro avg       0.73      0.72      0.71       600
weighted avg       0.73      0.71      0.71       600































